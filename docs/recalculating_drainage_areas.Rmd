---
title: "Little Shasta: Using FF Metrics"
output: 
    html_document:
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)

# libraries
library(sf)
library(tidyverse)
library(mapview)
library(tmap)
library(glue)
library(here)
mapviewOptions(fgb=FALSE)

```

As part of the CEFF case study of the Little Shasta, we identified a number of inaccuracies associated with the NHD watershed and streamline delineation.
In part, this was because canals have been treated as streamlines.
In particular, the canal which delivers water from Lake Shastina is routed as if it is a drainage to the Little Shasta.

## Recalculating Drainage Area & Streamlines

To address inaccuracies in the NHD flowlines and catchments associated with designation of canals as streams, the following steps were taken:

1.  We identified natural stream channel from artificial (canal) channels, and filtered all artificial segments out, and fixed mis-classified segments.
2.  Identify catchments based on the new clean streamline dataset, and create a revised catchment layer.
3.  Calculate the total drainage area for each NHD COMID segment that is affected by the revised catchment layer, and add this attribute information as a new column in the streamline layer. Retain the old drainage area associated with each NHD segment to facilitate comparisons.
4.  Calculate the difference between the two drainage areas, and adjust **magnitude** functional flow metrics by this percentage difference (e.g., if the adjusted drainage area is 20% smaller than the original value, the magnitude metrics are reduced by 20%).
5.  Compare **timing** and **duration** metrics from the canal segment which "connects" to the Little Shasta from the south with the same metrics associated with the segment immediately upstream on the mainstem Little Shasta.
6.  Compare the overall drainage area and magnitude ratios between the same upstream segment and canal junction segment to see if they are comparable (to get a sense of watershed differences such as elevation, geology, and soil type which influence the underlying models)

**Magnitude Metrics:**

-   `DS_Mag_50, DS_Mag_90, FA_Mag, Peak_10, Peak_5, Peak_2, SP_Mag, Wet_BFL_Mag_10, Wet_BFL_Mag_50`

## Step 1. Clean Streamlines

Identify and clean the streamlines.
Here we dropped all the canals and streamline network south of the Little Shasta River.
This map shows the original streamlines (light blue thin lines) vs. the cleaned streamlines (dark blue thick lines). In addition, it shows the COMID segment (green=3917948) that is upstream of the canal crossing (red segment=3917266). We can compare the predictions between these segments to assess differences in timing and duration.

```{r getData}

# gages
gages<- read_rds(file = "data_output/gages_lshasta.rds")

# path to database:
db <- glue("{here()}/data/nhdplus_little_shasta.gpkg")

# layers:
st_layers(db)

# cleaned little shasta streamlines
lshasta_clean <- read_sf(db, "lshasta_clean")

# GET NHD data

# full nhd flowlines and catchments with nhdplus attributes
flowlines <- read_sf(db, "NHDFlowline_Network") %>% 
    select(id:reachcode,ftype:streamcalc,fromnode:hydroseq,uplevelpat:dnhydroseq, areasqkm:divdasqkm)
flowlines_streamriver <- flowlines %>% filter(ftype=="StreamRiver")

nhd_catchment <- sf::read_sf(db, "CatchmentSP")
nhd_basin <- read_sf(db, "basin_lshasta") # this is nhd catchment basin
nhd_wb <- read_sf(db, "NHDWaterBody")
h10 <- read_sf(db, "h10_lshasta")
h12 <- read_sf(db, "h12_lshasta")

```

```{r makeBasicMap, eval=FALSE}

nhd_basin <- st_zm(nhd_basin, drop=TRUE)
h10 <- st_zm(h10, drop=TRUE)

ggplot() + 
    geom_sf(data=nhd_catchment, fill="slateblue", col=alpha("gray50", 0.5), lwd=0.2, alpha=0.25) +
    geom_sf(data=nhd_basin, fill=NA, col=alpha("black", 0.6), lwd=1.2) +
    geom_sf(data=h10, fill=alpha("darkblue", 0.1), col=alpha("darkblue",0.5), lwd=1)+
    geom_sf(data=flowlines, col="dodgerblue", lwd=0.5) +
    geom_sf(data=lshasta_clean, col="darkblue", lwd=1.2) +
    geom_sf(data=flowlines %>% filter(comid==3917948), col="green3", lwd=1.1) +
    geom_sf(data=flowlines %>% filter(comid==3917266), col="maroon", lwd=1.1) +
    theme_void() +
    ggspatial::annotation_north_arrow(location="br") +
    geom_sf_text(data=nhd_basin, aes(label="NHD Basin"), 
                 color=alpha("black", 0.6), fontface="bold")+
    geom_sf_text(data=h10, aes(label="HUC10"), fontface="bold",
                 nudge_x = -0.15, nudge_y = 0.03, color=alpha("darkblue", 0.5))+
    ggspatial::annotation_scale() +
    labs(title="Little Shasta NHD Layers", 
         subtitle="Subcatchments, original NHD flowlines (light blue), and cleaned NHD flowlines (darkblue)",
         caption = "Compare the upstream COMID 3917948 (green)\n with the canal diversion COMID 3917266 (red) to assess FFM differences")
#ggsave(filename = "figs/map_of_flowlines_existing_vs_cleaned.png", width = 8, height = 10, units="in", dpi=300)

```

```{r printBasicMap, out.width='100%'}

knitr::include_graphics(glue("{here()}/figs/map_of_flowlines_existing_vs_cleaned.png"))

```


### Pull FFC predictions for COMIDS

To compare the difference between the two `COMID`s, we can use the FFC R API to pull predictions for each segment, and then compare metrics.

**Upstream Segment**:

```{r ffc2Comids, eval=F}
library(ffcAPIClient)
ffctoken <- set_token(Sys.getenv("EFLOWS", ""))
ffcAPIClient::clean_account(ffctoken)

lsh_us <- ffcAPIClient::get_predicted_flow_metrics("3917948")
lsh_ds <- ffcAPIClient::get_predicted_flow_metrics("3917266")

# bind
lsh_preds <- bind_rows(lsh_us, lsh_ds) %>% arrange(metric)

# save
save(lsh_preds, file = "data_output/lshasta_stream_predictions_2comids.rda")
```

```{r dataTablePreds, eval=TRUE}
load(glue("{here()}/data_output/lshasta_stream_predictions_2comids.rda"))
DT::datatable(lsh_preds) %>% 
    DT::formatRound(columns=c('p10', 'p25', 'p50', 'p75', 'p90'), digits=1)
```


## Step 2. Clean Catchments

Next we cleaned the catchment layer to only include sub-catchments that are part of the cleaned flowline layer.
We can use the `st_intersection` function in addition to a filter to grab all the catchments we need.

```{r cleanCatch, echo=F}

# get spatial join to find intersects
catchment_ids <- st_intersects(nhd_catchment, lshasta_clean) # returns sgbp list
catchment_ids <- lengths(catchment_ids) > 0 # get list of matches
catchment_clean <- nhd_catchment[catchment_ids, ]

# need to add the few catchments that were missed:
catch_missed <- c("catchmentsp.2515923", #"catchmentsp.2515920", 
                  # these one seems odd not to include
                  "catchmentsp.2515839", "catchmentsp.2515810")

# add to cleaned set
catchment_clean <- nhd_catchment %>% filter(id %in% catch_missed) %>% bind_rows(., catchment_clean)

# make a map
mapview(lshasta_clean, lwd=3) + 
    mapview(nhd_catchment, col.regions="orange", alpha.regions=0.3, 
            color="darkorange", lwd=0.5, layer.name="NHD Catchments") +
    mapview(catchment_clean, col.regions="green", alpha.regions=0.3, 
            lwd=0.8, layer.name="Catchments to Keep") + 
    mapview(flowlines_streamriver, 
            #zcol="ftype",
            color="skyblue", 
            lwd=1.5, layer.name="Original 'Stream' Flowlines")

# write back out to gpkg
#st_write(catchment_clean, db, "lshasta_catchments_clean", delete_layer = TRUE)

```

### Merge Catchments

We need to make a new drainage area attributes using our clean flowline layer. First, we merge the catchments by dissolving `catchmentsp.2515923` with the mainstem catchment `catchmentsp.2515451` (COMID 10035874). 

```{r plotWatersheds, eval=T, echo=F}

plot(catchment_clean[catchment_clean$id=="catchmentsp.2515923",]$geom, col=alpha("gold2", 0.5), border = "gold2", lwd=1)
plot(catchment_clean[catchment_clean$id=="catchmentsp.2515451",]$geom, col = "skyblue", add=TRUE, border = "darkblue", lwd=0.4)
plot(catchment_clean$geom, add=TRUE, lty=2, border="gray")
```

We can dissolve using `st_union` of the two different pieces we want to merge.

```{r intersectSheds}

catchcombined <- st_union(catchment_clean[catchment_clean$id %in% c("catchmentsp.2515923"),], catchment_clean[catchment_clean$id %in% c("catchmentsp.2515451"),]) %>% 
    # keep original featureid
    rename(comid=featureid, featureid=featureid.1)

plot(catchcombined$geom, col=alpha("forestgreen", 0.5))
plot(catchment_clean$geom, add=T, lty=2, border="gray")

# filter out original catchments
catchment_temp <- catchment_clean %>% 
    filter(!id %in% c("catchmentsp.2515923", "catchmentsp.2515451"))

# bind back updated version
catchment_clean <- bind_rows(catchment_temp, catchcombined) %>% 
    # drop the duplicated cols
    select(-c(ends_with(".1")))

```


### Cleaned Catchment Map

Make a static map of just the cleaned version.

```{r cleanMap, eval=F}

# clean map
ggplot() + 
        geom_sf(data=catchment_clean, fill=alpha("dodgerblue", 0.5), col=alpha("black", .9), lwd=0.2, lty=3) +
        geom_sf(data=h10, fill="forestgreen", col=alpha("gray50", 0.7), lwd=1.2, alpha=0.25) +
        geom_sf(data=lshasta_clean, col="darkblue", lwd=1.2) +
        geom_sf(data=gages[c(2:3),], fill="skyblue", pch=21, size=4)+
        theme_void() +
        ggspatial::annotation_north_arrow(location="br") +
        ggspatial::annotation_scale() +
        labs(title="Little Shasta: Cleaned Catchment & Streamline Map", 
             subtitle=glue("NHDPlus subcatchments (dotted lines), \nHUC10 (thick gray), Gages (blue dot)"))

#ggsave(filename = "figs/map_of_cleaned_lshasta.png", width = 8, height = 10, units="in", dpi=300)

```

```{r printCleanMap, eval=T, echo=F, out.width='100%'}

knitr::include_graphics(glue("{here()}/figs/map_of_cleaned_lshasta.png"))

```

## Step 3. Calculate drainage area for each COMID segment

First we need to get the NHD flowline info for our selected COMID's, then we can calculate the updated drainage areas, and then add this data back to the layer.

Here we add one additional segment in the lower part of the watershed.

```{r flowlineTrim}

# get flowline data
flowlines_trim <- flowlines %>% filter(comid %in% lshasta_clean$comid) %>% 
    bind_rows(., flowlines %>% filter(comid==3917230))

mapview(catchment_clean, col.regions="blue2", alpha.regions=0.4, layer.name="Catchment<br>Area (sqkm)") +
        mapview(flowlines_trim, zcol="hydroseq", lwd=2, layer.name="Hydroseq ID", legend=F)

```

### Add Drainage Area Attributes

Now we have a cleaned dataset we can add the drainage area to each segment and cumulatively add them up. First we calculate the area of the watersheds and join the catchment attributes with the comids that are impacted by the cleaned watershed layer (basically everything downstream of `hydroseq=10035874`).

```{r, warning=FALSE, message=FALSE}

# first we calculate the area of each and add these attributes:
catchment_clean2 <- catchment_clean %>% 
    mutate(areasqkm_rev = as.numeric(units::set_units(st_area(geom), km^2))) %>% 
    relocate(areasqkm_rev, .after=areasqkm)

# next we bind the catchment attribute (updated area) to the flowlines
# everything is the same upstream of hydroseq 10035874
# filter first and then spatial join to get attribs

flowlines_trim2 <- flowlines_trim %>% 
    filter(hydroseq <=10035874) %>% 
    left_join(., st_drop_geometry(catchment_clean2[,c("featureid","areasqkm", "areasqkm_rev")]), by=c("comid"="featureid")) %>% 
    rename(areasqkm_catch=areasqkm.y) %>% 
    relocate(any_of(c("areasqkm_catch", "areasqkm_rev")), .before=totdasqkm) %>% 
    select(id:comid, ftype, lengthkm, hydroseq, contains("sqkm"))

# quick preview maps
mapview(catchment_clean2, alpha.regions=0.1, color="gray", legend=F) + mapview(flowlines_trim2, zcol="hydroseq", legend=F)

```

Next we arrange by the hydroseq ID, add an ordering column, and cumulatively add up the drainage area.

```{r, eval=T}

# check order
#flowlines_trim2 %>% arrange(desc(hydroseq)) %>% select(id:comid, lengthkm, hydroseq, contains("sqkm")) %>% View()

# everything is the same upstream of hydroseq 10035874, but only a slight difference in area in comid 3917198 (0.3)
df_da <- flowlines_trim2 %>% 
    arrange(desc(hydroseq)) %>% 
    mutate(dasqkm1 =
        lag(areasqkm_rev) + areasqkm_rev) %>% 
    select(id:comid, lengthkm, hydroseq, contains("sqkm")) %>% 
    mutate(dasqkm = case_when(
        is.na(dasqkm1) ~ 155.3292,
        TRUE ~ lag(dasqkm1) + dasqkm1
    ))

# then fill the NA's
df_da <- df_da %>% 
    arrange(desc(hydroseq)) %>% 
    mutate(dasqkm2 = if_else(is.na(dasqkm), 10.7033+155.3292)
View(df_da)
```

## Timing & Duration Metrics

One question is what to do about the change in watershed area and the impact on timing and duration metrics.

 - To account for change in drainage area in the *cleaned* Little Shasta, we can compare timing metrics from COMID=3917948 with the segment just upstream of the canal join (COMID=3917266), at COMID junction between 3917950 and 3917948.
 - Use the timing from upstream segment (3917948) and propagate downstream (Does appear they are different).

 - Could scale magnitude based on overall drainage area to a segment (canal) vs. overall to just upstream segment.
